@article{AguettazLoeliger2024,
  bibtex_show={true},
  abbr={arXiv},
  title={Continuous-Time Neural Networks Can Stably Memorize Random Spike Trains}, 
  author={Hugo Aguettaz and Hans-Andrea Loeliger},
  year={2024},
  journal={arXiv},
  url={https://arxiv.org/abs/2408.01166}, 
  pdf={https://arxiv.org/pdf/2408.01166.pdf},
  abstract={The paper explores the capability of continuous-time recurrent neural networks to store and recall precisely timed spike patterns. We show (by numerical experiments) that this is indeed possible: within some range of parameters, any random score of spike trains (for all neurons in the network) can be robustly memorized and autonomously reproduced with stable accurate relative timing of all spikes, with probability close to one. We also demonstrate associative recall under noisy conditions.
  In these experiments, the required synaptic weights are computed offline, to satisfy a template that encourages temporal stability.},
  selected={true}
}

@article{AguettazetAl2021,
  abbr={arXiv},
  bibtex_show={true},
  title={ChebLieNet: Invariant Spectral Graph NNs Turned Equivariant by Riemannian Geometry on Lie Groups}, 
  author={Hugo Aguettaz and Erik J. Bekkers and MichaÃ«l Defferrard},
  year={2021},
  journal={arXiv},
  url={https://arxiv.org/abs/2111.12139}, 
  pdf={https://arxiv.org/pdf/2111.12139.pdf},
  abstract={We introduce ChebLieNet, a group-equivariant method on (anisotropic) manifolds. Surfing on the success of graph- and group-based neural networks, we take advantage of the recent developments in the geometric deep learning field to derive a new approach to exploit any anisotropies in data. Via discrete approximations of Lie groups, we develop a graph neural network made of anisotropic convolutional layers (Chebyshev convolutions), spatial pooling and unpooling layers, and global pooling layers. Group equivariance is achieved via equivariant and invariant operators on graphs with anisotropic left-invariant Riemannian distance-based affinities encoded on the edges. Thanks to its simple form, the Riemannian metric can model any anisotropies, both in the spatial and orientation domains. This control on anisotropies of the Riemannian metrics allows to balance equivariance (anisotropic metric) against invariance (isotropic metric) of the graph convolution layers. Hence we open the doors to a better understanding of anisotropic properties. Furthermore, we empirically prove the existence of (data-dependent) sweet spots for anisotropic parameters on CIFAR10. This crucial result is evidence of the benefice we could get by exploiting anisotropic properties in data. We also evaluate the scalability of this approach on STL10 (image data) and ClimateNet (spherical data), showing its remarkable adaptability to diverse tasks.},
  }